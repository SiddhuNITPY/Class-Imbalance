{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiddhuNITPY/Class-Imbalance/blob/main/XOR_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4wqm5fdjHU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948ec648-e04e-49f1-864b-2b4b5c69dc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to Hidden - Weights\n",
            "[[ 5.5387111  -1.48589032  2.81646718 -6.66007311]\n",
            " [-6.66611459 -1.61116969  2.91553984  5.54420729]]\n",
            "Input to Hidden - Bias\n",
            "[[-2.81734979 -0.24671565 -0.47624113 -2.82012927]]\n",
            " Hidden to Output - Weights\n",
            "[[10.01805101]\n",
            " [-1.55176602]\n",
            " [ 0.83981415]\n",
            " [ 9.98085692]]\n",
            " Hidden to Output - Bias\n",
            "[[-5.42371686]]\n"
          ]
        }
      ],
      "source": [
        "                                            # 2-Bit XOR Problem\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid activation function\n",
        "sigmoid_derivative = lambda x: x * (1 - x)\n",
        "\n",
        "# Define the input-output pairs for the XOR function\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Define the weights for the input layer and the hidden layer, as well as the biases\n",
        "weights_ih = np.random.normal(0.0, pow(2, -0.5), (2, 4))\n",
        "biases_ih = np.zeros((1, 4))\n",
        "weights_ho = np.random.normal(0.0, pow(4, -0.5), (4, 1))\n",
        "biases_ho = np.zeros((1, 1))\n",
        "\n",
        "# Create empty lists to store weight values for each layer at each iteration during training\n",
        "weights_ih_history = []\n",
        "biases_ih_history = []\n",
        "weights_ho_history = []\n",
        "biases_ho_history = []\n",
        "\n",
        "# Train the neural network using backpropagation\n",
        "for i in range(10000):\n",
        "    # Forward pass\n",
        "    hidden_inputs = np.dot(X, weights_ih) + biases_ih\n",
        "    hidden_outputs = sigmoid(hidden_inputs)\n",
        "    output_inputs = np.dot(hidden_outputs, weights_ho) + biases_ho\n",
        "    output_outputs = sigmoid(output_inputs)\n",
        "\n",
        "    # Calculate the error and the delta for the output layer\n",
        "    output_error = Y - output_outputs\n",
        "    output_delta = output_error * sigmoid_derivative(output_outputs)\n",
        "\n",
        "    # Calculate the error and the delta for the hidden layer\n",
        "    hidden_error = np.dot(output_delta, weights_ho.T)\n",
        "    hidden_delta = hidden_error * sigmoid_derivative(hidden_outputs)\n",
        "\n",
        "    # Update the weights and biases for the hidden layer and the output layer\n",
        "    weights_ho += np.dot(hidden_outputs.T, output_delta)\n",
        "    biases_ho += np.sum(output_delta, axis=0)\n",
        "    weights_ih += np.dot(X.T, hidden_delta)\n",
        "    biases_ih += np.sum(hidden_delta, axis=0)\n",
        "\n",
        "    # Append the current weight and bias values for each layer to the history lists\n",
        "    weights_ih_history.append(weights_ih.copy())\n",
        "    biases_ih_history.append(biases_ih.copy())\n",
        "    weights_ho_history.append(weights_ho.copy())\n",
        "    biases_ho_history.append(biases_ho.copy())\n",
        "\n",
        "# Print the final weights and biases for the input layer and the hidden layer\n",
        "print(\"Input to Hidden - Weights\")\n",
        "print(weights_ih)\n",
        "print(\"Input to Hidden - Bias\")\n",
        "print(biases_ih)\n",
        "print(\" Hidden to Output - Weights\")\n",
        "print(weights_ho)\n",
        "print(\" Hidden to Output - Bias\")\n",
        "print(biases_ho)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mkPCXNjoOhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad67691-591c-4bb9-ad4e-180a6d25006b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output\n",
            "[[0.00951932]]\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network using some input values\n",
        "test_input = np.array([[0.01, 0.01]])\n",
        "test_hidden = sigmoid(np.dot(test_input, weights_ih) + biases_ih)\n",
        "test_output = sigmoid(np.dot(test_hidden, weights_ho) + biases_ho)\n",
        "print(\"Test output\")\n",
        "print(test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_pK9rAHq7Bm",
        "outputId": "2cdfd6e1-04ca-43d9-c15b-37ce2936b95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -9.85809707   3.69312245 -10.32651995 -10.11330684]]\n",
            "[[-2.43637974  3.75869252 -8.27355005 -8.11868096]]\n",
            "[[0.9195597  0.02278304 0.99974489 0.99970217]]\n",
            "[[-0.57870333]]\n",
            "[[4.46217557]]\n",
            "Final Output\n",
            "[[0.01140565]]\n"
          ]
        }
      ],
      "source": [
        "# Proof of result\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(x))\n",
        "\n",
        "test_input = np.array([[1, 1]])\n",
        "\n",
        "weights_ih = np.array([[ 4.9135242, -1.7484562,  5.15949378,    5.06430999],\n",
        "                       [ 4.94457287, -1.94466625, 5.16702617, 5.04899685]])\n",
        "\n",
        "biases_ih = np.array([[-7.42171733,  -0.06557007, -2.0529699, -1.99462588]])\n",
        "\n",
        "weights_ho = np.array([[-11.5610553],\n",
        "                       [ -1.77568114], \n",
        "                       [  5.73963433],\n",
        "                       [  5.51371142]])\n",
        "\n",
        "biases_ho = np.array([[-5.0408789]])\n",
        "\n",
        "\n",
        "weights_ih= weights_ih * -1\n",
        "weights_ho= weights_ho * -1\n",
        "\n",
        "biases_ih= biases_ih * -1\n",
        "biases_ho= biases_ho * -1\n",
        "\n",
        "res = np.dot(test_input, weights_ih)\n",
        "print (res)\n",
        "\n",
        "res = res + biases_ih\n",
        "print (res)\n",
        "\n",
        "res = sigmoid (res)\n",
        "print (res)\n",
        "\n",
        "res= np.dot(res, weights_ho) \n",
        "print(res)\n",
        "\n",
        "res = res+ biases_ho\n",
        "print(res)\n",
        "\n",
        "res= sigmoid(res)\n",
        "print (\"Final Output\") \n",
        "print (res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mAGaKWE41tW",
        "outputId": "a38c2333-8664-42b4-b830-d9301949ca6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input to Hidden - Weights\n",
            "[[-3.66557405 -2.47512775  5.84017589  5.69469026]\n",
            " [-4.04110477 -1.59710956 -6.81638032 -4.24533012]\n",
            " [ 5.70582128 -2.29718044  5.81317079 -3.85573787]]\n",
            "Input to Hidden - Bias\n",
            "[[ 0.98907557  3.01655314 -2.38196853  1.20743707]]\n",
            " Hidden to Output - Weights\n",
            "[[-8.50800737]\n",
            " [ 5.91985063]\n",
            " [ 9.45041901]\n",
            " [-8.26576048]]\n",
            " Hidden to Output - Bias\n",
            "[[0.848123]]\n",
            "Test output\n",
            "[[0.9929462]]\n"
          ]
        }
      ],
      "source": [
        "                                                              # 3-bit XOR problem\n",
        "  import numpy as np\n",
        "\n",
        "  # Define the sigmoid activation function\n",
        "  sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "  # Define the derivative of the sigmoid activation function\n",
        "  sigmoid_derivative = lambda x: x * (1 - x)\n",
        "\n",
        "  # Define the input-output pairs for the XOR function\n",
        "  X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
        "  Y = np.array([[0], [1], [1], [0], [1], [0], [0], [1]])\n",
        "\n",
        "  # Define the weights for the input layer and the hidden layer, as well as the biases\n",
        "  weights_ih = np.random.normal(0.0, pow(3, -0.5), (3, 4))\n",
        "  biases_ih = np.zeros((1, 4))\n",
        "  weights_ho = np.random.normal(0.0, pow(4, -0.5), (4, 1))\n",
        "  biases_ho = np.zeros((1, 1))\n",
        "\n",
        "  # Create empty lists to store weight values for each layer at each iteration during training\n",
        "  weights_ih_history = []\n",
        "  biases_ih_history = []\n",
        "  weights_ho_history = []\n",
        "  biases_ho_history = []\n",
        "\n",
        "  # Train the neural network using backpropagation\n",
        "  for i in range(10000):\n",
        "      # Forward pass\n",
        "      hidden_inputs = np.dot(X, weights_ih) + biases_ih\n",
        "      hidden_outputs = sigmoid(hidden_inputs)\n",
        "      output_inputs = np.dot(hidden_outputs, weights_ho) + biases_ho\n",
        "      output_outputs = sigmoid(output_inputs)\n",
        "\n",
        "      # Calculate the error and the delta for the output layer\n",
        "      output_error = Y - output_outputs\n",
        "      output_delta = output_error * sigmoid_derivative(output_outputs)\n",
        "\n",
        "      # Calculate the error and the delta for the hidden layer\n",
        "      hidden_error = np.dot(output_delta, weights_ho.T)\n",
        "      hidden_delta = hidden_error * sigmoid_derivative(hidden_outputs)\n",
        "\n",
        "      # Update the weights and biases for the hidden layer and the output layer\n",
        "      weights_ho += np.dot(hidden_outputs.T, output_delta)\n",
        "      biases_ho += np.sum(output_delta, axis=0)\n",
        "      weights_ih += np.dot(X.T, hidden_delta)\n",
        "      biases_ih += np.sum(hidden_delta, axis=0)\n",
        "\n",
        "      # Append the current weight and bias values for each layer to the history lists\n",
        "      weights_ih_history.append(weights_ih.copy())\n",
        "      biases_ih_history.append(biases_ih.copy())\n",
        "      weights_ho_history.append(weights_ho.copy())\n",
        "      biases_ho_history.append(biases_ho.copy())\n",
        "\n",
        "  # Print the final weights and biases for the input layer and the hidden layer\n",
        "  print(\"Input to Hidden - Weights\")\n",
        "  print(weights_ih)\n",
        "  print(\"Input to Hidden - Bias\")\n",
        "  print(biases_ih)\n",
        "  print(\" Hidden to Output - Weights\")\n",
        "  print(weights_ho)\n",
        "  print(\" Hidden to Output - Bias\")\n",
        "  print(biases_ho)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbD3VjW761vx",
        "outputId": "fb0a9f6c-34f3-4c78-fa26-3c41768a7319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test output\n",
            "[[0.99619635]]\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network using some input values\n",
        "test_input = np.array([[1, 1, 1]])\n",
        "test_hidden = sigmoid(np.dot(test_input, weights_ih) + biases_ih)\n",
        "test_output = sigmoid(np.dot(test_hidden, weights_ho) + biases_ho)\n",
        "print(\"Test output\")\n",
        "print(test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtH9MsZ8Dze",
        "outputId": "459f3364-c9d0-49dd-8b2e-93ab68c4bda0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input to Hidden - Weights\n",
            "[[ 0.22258317 -1.64611813 -6.04125426 -3.80689101]\n",
            " [-0.47417356 -1.14240691  5.39908009  4.78469569]\n",
            " [-0.55382551  4.79219335  5.08536284 -2.94637848]]\n",
            "Input to Hidden1 - Bias\n",
            "[[ 0.04396799 -1.05984152 -2.19727444  0.94229088]]\n",
            "Hidden 1 to Hidden 2 - Weights\n",
            "[[-0.92404918 -0.89433747 -0.55490736  0.4618399 ]\n",
            " [ 3.61233801  1.83412709  1.41325142 -4.07557902]\n",
            " [-4.09583007 -1.45624378 -1.68829449  4.54437311]\n",
            " [ 4.13271552  0.83305376  1.86823097 -4.03558174]]\n",
            "Hidden 1 to Hidden 2 - Bias\n",
            "[[-1.44946138 -0.25460716 -0.58908113  1.55836337]]\n",
            " Hidden to Output - Weights\n",
            "[[-6.98379036]\n",
            " [-2.13319593]\n",
            " [-2.5918603 ]\n",
            " [ 8.0610889 ]]\n",
            " Hidden to Output - Bias\n",
            "[[1.71911078]]\n"
          ]
        }
      ],
      "source": [
        "                                        # 3-bit XOR problem (with 2 hidden layer)\n",
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid activation function\n",
        "sigmoid_derivative = lambda x: x * (1 - x)\n",
        "\n",
        "# Define the input-output pairs for the XOR function\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
        "Y = np.array([[0], [1], [1], [0], [1], [0], [0], [1]])\n",
        "\n",
        "# Define the weights for the input layer and the hidden layer, as well as the biases\n",
        "weights_ih = np.random.normal(0.0, pow(3, -0.5), (3, 4))\n",
        "biases_ih = np.zeros((1, 4))\n",
        "weights_h1h2 = np.random.normal(0.0, pow(4, -0.5), (4, 4))\n",
        "biases_h1h2 = np.zeros((1, 4))\n",
        "weights_ho = np.random.normal(0.0, pow(4, -0.5), (4, 1))\n",
        "biases_ho = np.zeros((1, 1))\n",
        "\n",
        "# Create empty lists to store weight values for each layer at each iteration during training\n",
        "weights_ih_history = []\n",
        "biases_ih_history = []\n",
        "weights_h1h2_history = []\n",
        "biases_h1h2_history = []\n",
        "weights_ho_history = []\n",
        "biases_ho_history = []\n",
        "\n",
        "# Train the neural network using backpropagation\n",
        "for i in range(10000):\n",
        "    # Forward pass\n",
        "    hidden1_inputs = np.dot(X, weights_ih) + biases_ih\n",
        "    hidden1_outputs = sigmoid(hidden1_inputs)\n",
        "    hidden2_inputs = np.dot(hidden1_outputs, weights_h1h2) + biases_h1h2\n",
        "    hidden2_outputs = sigmoid(hidden2_inputs)\n",
        "    output_inputs = np.dot(hidden2_outputs, weights_ho) + biases_ho\n",
        "    output_outputs = sigmoid(output_inputs)\n",
        "\n",
        "    # Calculate the error and the delta for the output layer\n",
        "    output_error = Y - output_outputs\n",
        "    output_delta = output_error * sigmoid_derivative(output_outputs)\n",
        "\n",
        "    # Calculate the error and the delta for the second hidden layer\n",
        "    hidden2_error = np.dot(output_delta, weights_ho.T)\n",
        "    hidden2_delta = hidden2_error * sigmoid_derivative(hidden2_outputs)\n",
        "\n",
        "    # Calculate the error and the delta for the first hidden layer\n",
        "    hidden1_error = np.dot(hidden2_delta, weights_h1h2.T)\n",
        "    hidden1_delta = hidden1_error * sigmoid_derivative(hidden1_outputs)\n",
        "\n",
        "    # Update the weights and biases for the hidden layer 1, hidden layer 2, and the output layer\n",
        "    weights_ho += np.dot(hidden2_outputs.T, output_delta)\n",
        "    biases_ho += np.sum(output_delta, axis=0)\n",
        "    weights_h1h2 += np.dot(hidden1_outputs.T, hidden2_delta)\n",
        "    biases_h1h2 += np.sum(hidden2_delta, axis=0)\n",
        "    weights_ih += np.dot(X.T, hidden1_delta)\n",
        "    biases_ih += np.sum(hidden1_delta, axis=0)\n",
        "\n",
        "    # Append the current weight and bias values for each layer to the history lists\n",
        "    weights_ih_history.append(weights_ih.copy())\n",
        "    biases_ih_history.append(biases_ih.copy())\n",
        "    weights_h1h2_history.append(weights_h1h2.copy())\n",
        "    biases_h1h2_history.append(biases_h1h2.copy())\n",
        "    weights_ho_history.append(weights_ho.copy())\n",
        "    biases_ho_history.append(biases_ho.copy())\n",
        "\n",
        "# Print the final weights and biases for the input layer and the hidden layer\n",
        "print(\"Input to Hidden 1 - Weights\")\n",
        "print(weights_ih)\n",
        "print(\"Input to Hidden 1 - Bias\")\n",
        "print(biases_ih)\n",
        "print(\"Hidden 1 to Hidden 2 - Weights\")\n",
        "print(weights_h1h2)\n",
        "print(\"Hidden 1 to Hidden 2 - Bias\")\n",
        "print(biases_h1h2)\n",
        "print(\" Hidden 2 to Output - Weights\")\n",
        "print(weights_ho)\n",
        "print(\" Hidden 2 to Output - Bias\")\n",
        "print(biases_ho)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I4nyNVbJyLE",
        "outputId": "3b798549-5e90-4e8d-85d3-1d1eca98ca9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test output\n",
            "[[0.99730021]]\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network using some input values\n",
        "test_input = np.array([[1, 1, 1]])\n",
        "test_hidden1 = sigmoid(np.dot(test_input, weights_ih) + biases_ih)\n",
        "test_hidden2 = sigmoid(np.dot(test_hidden1, weights_h1h2) + biases_h1h2)\n",
        "test_output = sigmoid(np.dot(test_hidden2, weights_ho) + biases_ho)\n",
        "print(\"Test output\")\n",
        "print(test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "omcUG2dM0hrg",
        "outputId": "30389a89-9656-4d02-e965-f3a6dae4e487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-6.98379036\n",
            "8.0610889\n",
            "32\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Define the weights\n",
        "W1 = np.array([[ 0.22258317, -1.64611813, -6.04125426, -3.80689101],\n",
        "               [-0.47417356, -1.14240691,  5.39908009,  4.78469569],\n",
        "               [-0.55382551,  4.79219335,  5.08536284, -2.94637848]])\n",
        "\n",
        "W2 = np.array([[-0.92404918, -0.89433747, -0.55490736,  0.4618399 ],\n",
        "               [ 3.61233801,  1.83412709,  1.41325142, -4.07557902],\n",
        "               [-4.09583007, -1.45624378, -1.68829449,  4.54437311],\n",
        "               [ 4.13271552,  0.83305376,  1.86823097, -4.03558174]])\n",
        "\n",
        "W3 = np.array([[-6.98379036],\n",
        "               [-2.13319593],\n",
        "               [-2.5918603 ],\n",
        "               [ 8.0610889 ]])\n",
        "\n",
        "# Reshape the weights to a single vector\n",
        "weights = np.concatenate([W1.flatten(), W2.flatten(), W3.flatten()])\n",
        "\n",
        "print(np.min(weights))\n",
        "print(np.max(weights))\n",
        "print(len(weights))\n",
        "\n",
        "# Plot the distribution of each weight matrix\n",
        "plt.hist(W1.flatten(), bins=20, label='W1')\n",
        "plt.hist(W2.flatten(), bins=20, label='W2')\n",
        "plt.hist(W3.flatten(), bins=20, label='W3')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY1d8epXN8tzn2EfBJ5g6l",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}